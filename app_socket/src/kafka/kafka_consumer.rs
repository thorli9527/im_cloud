use std::sync::Arc;
use dashmap::DashMap;
use bytes::Bytes;
use once_cell::sync::OnceCell;
use uuid::Uuid;
use serde::Deserialize;
use anyhow::{Result, anyhow};

use rdkafka::config::ClientConfig;
use rdkafka::consumer::{CommitMode, Consumer, StreamConsumer};
use rdkafka::message::{Message, OwnedMessage};

use crate::manager::socket_manager::SocketManager;
use common::config::KafkaConfig;

type MessageId = String;

/// Kafka æ¶ˆæ¯åˆ†å‘ç»“æ„ä½“
#[derive(Debug, Deserialize)]
pub struct DispatchMessage {
    pub to: String,                     // æ¥æ”¶è€…ç”¨æˆ· ID
    pub payload: String,               // æ¶ˆæ¯ä½“ï¼ˆJSON/Proto åºåˆ—åŒ–åçš„å­—ç¬¦ä¸²ï¼‰
    pub message_id: Option<MessageId>, // ç”¨äºå®¢æˆ·ç«¯ç¡®è®¤çš„å”¯ä¸€æ ‡è¯†
}

/// Kafka æ¶ˆæ¯å…ƒæ•°æ®ï¼ˆç”¨äº ACK è¿½è¸ªï¼‰
pub struct PendingMeta {
    pub topic: String,
    pub partition: i32,
    pub offset: i64,
}

/// å…¨å±€æœªç¡®è®¤æ¶ˆæ¯æ˜ å°„ï¼ˆmsg_id -> å…ƒä¿¡æ¯ï¼‰
static PENDING_ACKS: OnceCell<Arc<DashMap<MessageId, PendingMeta>>> = OnceCell::new();

pub fn get_pending_acks() -> Arc<DashMap<MessageId, PendingMeta>> {
    PENDING_ACKS.get_or_init(|| Arc::new(DashMap::new())).clone()
}

/// å…¨å±€ Kafka æ¶ˆè´¹è€…å•ä¾‹
static CONSUMER: OnceCell<Arc<StreamConsumer>> = OnceCell::new();

/// è·å– Kafka æ¶ˆè´¹è€…å®ä¾‹
pub fn get_consumer() -> Option<Arc<StreamConsumer>> {
    CONSUMER.get().cloned()
}

/// å¯åŠ¨ Kafka æ¶ˆè´¹å¾ªç¯
pub async fn start_consumer(
    kafka_cfg: KafkaConfig,
    socket_manager: Arc<SocketManager>,
) -> Result<()> {
    let consumer: StreamConsumer = ClientConfig::new()
        .set("group.id", "im-dispatch-group")
        .set("bootstrap.servers", &kafka_cfg.brokers)
        .set("enable.auto.commit", "false") // æ‰‹åŠ¨æäº¤ offset
        .create()?;

    consumer.subscribe(&[&kafka_cfg.topic_single, &kafka_cfg.topic_group])?;
    log::info!("âœ… Kafka æ¶ˆè´¹è€…å·²å¯åŠ¨ï¼Œè®¢é˜…ä¸»é¢˜ï¼š{}, {}",
        kafka_cfg.topic_single, kafka_cfg.topic_group);

    let arc_consumer = Arc::new(consumer);
    if CONSUMER.set(arc_consumer.clone()).is_err() {
        log::warn!("âš ï¸ Kafka CONSUMER å·²åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤è®¾ç½®");
    }

    loop {
        match arc_consumer.recv().await {
            Ok(msg) => {
                let owned = msg.detach();
                if let Err(e) = handle_kafka_message(owned, &socket_manager).await {
                    log::error!("âŒ Kafka æ¶ˆæ¯å¤„ç†å¤±è´¥: {:?}", e);
                }
            }
            Err(e) => {
                log::error!("âŒ Kafka æ¶ˆè´¹é”™è¯¯: {:?}", e);
            }
        }
    }
}

/// å¤„ç†å•æ¡ Kafka æ¶ˆæ¯
async fn handle_kafka_message(
    msg: OwnedMessage,
    socket_manager: &Arc<SocketManager>,
) -> Result<()> {
    let payload = msg.payload_view::<str>()
        .ok_or_else(|| anyhow!("Kafka æ¶ˆæ¯ä¸ºç©º"))?
        .map_err(|e| anyhow!("Kafka æ¶ˆæ¯è§£ç å¤±è´¥: {:?}", e))?;

    let dispatch: DispatchMessage = serde_json::from_str(payload)
        .map_err(|e| anyhow!("æ¶ˆæ¯ JSON ååºåˆ—åŒ–å¤±è´¥: {:?}", e))?;

    let message_id = dispatch.message_id.unwrap_or_else(|| Uuid::new_v4().to_string());

    // ç¼“å­˜ç”¨äºåç»­ ACK
    get_pending_acks().insert(message_id.clone(), PendingMeta {
        topic: msg.topic().to_string(),
        partition: msg.partition(),
        offset: msg.offset(),
    });

    let bytes = Bytes::from(dispatch.payload);
    match socket_manager.send_to_user(&dispatch.to, bytes, None) {
        Ok(_) => {
            log::info!("ğŸ“¨ æˆåŠŸæ¨é€æ¶ˆæ¯ç»™ç”¨æˆ· [{}]", dispatch.to);
        }
        Err(e) => {
            log::warn!("âš ï¸ æ¨é€å¤±è´¥ [{}]: {:?}", dispatch.to, e);
        }
    }

    Ok(())
}
